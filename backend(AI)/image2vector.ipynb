{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcbaf535-9820-4424-bc55-87633fc02a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e21eff1-306b-4dec-830e-40a647f663e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c4696b-e269-4bec-8302-74ea918083b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>description</th>\n",
       "      <th>phash</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>genre_count</th>\n",
       "      <th>subset</th>\n",
       "      <th>artist_ko</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract_Expressionism/aaron-siskind_acolman-1...</td>\n",
       "      <td>aaron siskind</td>\n",
       "      <td>['Abstract Expressionism']</td>\n",
       "      <td>acolman-1-1955</td>\n",
       "      <td>bebbeb018a7d80a8</td>\n",
       "      <td>1922</td>\n",
       "      <td>1382</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>아론 시스킨드</td>\n",
       "      <td>acolman-1</td>\n",
       "      <td>1955.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract_Expressionism/aaron-siskind_chicago-6...</td>\n",
       "      <td>aaron siskind</td>\n",
       "      <td>['Abstract Expressionism']</td>\n",
       "      <td>chicago-6-1961</td>\n",
       "      <td>d7d0781be51fc00e</td>\n",
       "      <td>1382</td>\n",
       "      <td>1746</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>아론 시스킨드</td>\n",
       "      <td>chicago-6</td>\n",
       "      <td>1961.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract_Expressionism/aaron-siskind_glouceste...</td>\n",
       "      <td>aaron siskind</td>\n",
       "      <td>['Abstract Expressionism']</td>\n",
       "      <td>gloucester-16a-1944</td>\n",
       "      <td>9f846e5a6c639325</td>\n",
       "      <td>1382</td>\n",
       "      <td>1857</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>아론 시스킨드</td>\n",
       "      <td>gloucester-16a</td>\n",
       "      <td>1944.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract_Expressionism/aaron-siskind_jerome-ar...</td>\n",
       "      <td>aaron siskind</td>\n",
       "      <td>['Abstract Expressionism']</td>\n",
       "      <td>jerome-arizona-1949</td>\n",
       "      <td>a5d691f85ac5e4d0</td>\n",
       "      <td>1382</td>\n",
       "      <td>1849</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>아론 시스킨드</td>\n",
       "      <td>jerome-arizona</td>\n",
       "      <td>1949.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abstract_Expressionism/aaron-siskind_kentucky-...</td>\n",
       "      <td>aaron siskind</td>\n",
       "      <td>['Abstract Expressionism']</td>\n",
       "      <td>kentucky-4-1951</td>\n",
       "      <td>880df359e6b11db1</td>\n",
       "      <td>1382</td>\n",
       "      <td>1625</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>아론 시스킨드</td>\n",
       "      <td>kentucky-4</td>\n",
       "      <td>1951.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename         artist  \\\n",
       "0  Abstract_Expressionism/aaron-siskind_acolman-1...  aaron siskind   \n",
       "1  Abstract_Expressionism/aaron-siskind_chicago-6...  aaron siskind   \n",
       "2  Abstract_Expressionism/aaron-siskind_glouceste...  aaron siskind   \n",
       "3  Abstract_Expressionism/aaron-siskind_jerome-ar...  aaron siskind   \n",
       "4  Abstract_Expressionism/aaron-siskind_kentucky-...  aaron siskind   \n",
       "\n",
       "                        genre          description             phash  width  \\\n",
       "0  ['Abstract Expressionism']       acolman-1-1955  bebbeb018a7d80a8   1922   \n",
       "1  ['Abstract Expressionism']       chicago-6-1961  d7d0781be51fc00e   1382   \n",
       "2  ['Abstract Expressionism']  gloucester-16a-1944  9f846e5a6c639325   1382   \n",
       "3  ['Abstract Expressionism']  jerome-arizona-1949  a5d691f85ac5e4d0   1382   \n",
       "4  ['Abstract Expressionism']      kentucky-4-1951  880df359e6b11db1   1382   \n",
       "\n",
       "   height  genre_count subset artist_ko           title    year  \n",
       "0    1382            1  train   아론 시스킨드       acolman-1  1955.0  \n",
       "1    1746            1  train   아론 시스킨드       chicago-6  1961.0  \n",
       "2    1857            1  train   아론 시스킨드  gloucester-16a  1944.0  \n",
       "3    1849            1  train   아론 시스킨드  jerome-arizona  1949.0  \n",
       "4    1625            1  train   아론 시스킨드      kentucky-4  1951.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"final_wikiart_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2487e792-4b77-4409-a0a5-824ab9b25cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed8815a-0f20-4640-bfe2-c54274e38621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduce_ratio=4):\n",
    "        super(SEBlock, self).__init__()\n",
    "        reduced_channels = in_channels // reduce_ratio\n",
    "        self.fc1 = nn.Conv2d(in_channels, reduced_channels, kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(reduced_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        se = F.adaptive_avg_pool2d(x, 1)\n",
    "        se = torch.relu(self.fc1(se))\n",
    "        se = torch.sigmoid(self.fc2(se))\n",
    "        return x * se\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion, stride, se_ratio=0.25):\n",
    "        super(MBConv, self).__init__()\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "        mid_channels = in_channels * expansion\n",
    "\n",
    "        self.expand_conv = nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
    "        self.deptwise_conv = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=stride, padding=1, groups=mid_channels, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(mid_channels)\n",
    "        self.se = SEBlock(mid_channels, reduce_ratio=int(1 / se_ratio))\n",
    "        self.project_conv = nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.act(self.bn1(self.expand_conv(x)))\n",
    "        x = self.act(self.bn2(self.deptwise_conv(x)))\n",
    "        x = self.se(x)\n",
    "        x = self.bn3(self.project_conv(x))\n",
    "        if self.use_residual:\n",
    "            x += residual\n",
    "        return x\n",
    "\n",
    "\n",
    "class FusedMBConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion, stride):\n",
    "        super(FusedMBConv, self).__init__()\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "        mid_channels = in_channels * expansion\n",
    "\n",
    "        self.expand_conv = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=stride, padding=1, bias=False) if expansion != 1 else None\n",
    "        self.bn1 = nn.BatchNorm2d(mid_channels if expansion != 1 else in_channels)\n",
    "        self.project_conv = nn.Conv2d(mid_channels if expansion != 1 else in_channels, out_channels, kernel_size=1 if expansion != 1 else 3, stride=1, padding=1 if expansion == 1 else 0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.expand_conv:\n",
    "            x = self.act(self.bn1(self.expand_conv(x)))\n",
    "        else:\n",
    "            x = self.act(self.bn1(x))\n",
    "        x = self.bn2(self.project_conv(x))\n",
    "        if self.use_residual:\n",
    "            x += residual\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=27):\n",
    "        super(EfficientNetV2, self).__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 24, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.block_config = [\n",
    "            (FusedMBConv, 2, 24, 24, 1, 1),\n",
    "            (FusedMBConv, 4, 24, 48, 4, 2),\n",
    "            (FusedMBConv, 4, 48, 64, 4, 2),\n",
    "            (MBConv, 6, 64, 128, 4, 2),\n",
    "            (MBConv, 9, 128, 160, 6, 1),\n",
    "            (MBConv, 15, 160, 256, 6, 2)\n",
    "        ]\n",
    "\n",
    "        layers = []\n",
    "        for block, repeats, in_channels, out_channels, expansion, stride in self.block_config:\n",
    "            for i in range(repeats):\n",
    "                if i == 0:\n",
    "                    layers.append(block(in_channels, out_channels, expansion, stride))\n",
    "                else:\n",
    "                    layers.append(block(out_channels, out_channels, expansion, 1))\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(256, 1280, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.SiLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5737a3d-39b4-4615-ad7d-b8007fabc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path='path', device='cuda'):\n",
    "    model = EfficientNetV2(num_classes=27)\n",
    "    model.load_state_dict(torch.load(path, map_location=device, weights_only=True))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "100f150c-b245-4edc-88f5-fc0700392929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_transform_image(artwork_path, transform, device):\n",
    "    if not os.path.exists(artwork_path):\n",
    "        return np.zeros((512,))\n",
    "    img = Image.open(artwork_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0)  # CPU에서 변환\n",
    "    return img_tensor  # GPU로 옮기지 않음\n",
    "\n",
    "def compute_gallery_vector_batch(artworks, model, device, num_workers=4, batch_size=4):\n",
    "    vectors = []\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    imgs = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        img_futures = [executor.submit(load_and_transform_image, os.path.join(artwork), transform, device) for artwork in artworks]\n",
    "        \n",
    "        for future in tqdm(img_futures, desc=\"Loading and transforming images\"):\n",
    "            img_tensor = future.result()\n",
    "            if isinstance(img_tensor, torch.Tensor):\n",
    "                imgs.append(img_tensor)\n",
    "\n",
    "    if len(imgs) > 0:\n",
    "        for i in tqdm(range(0, len(imgs), batch_size), desc=\"Loading Transfer vectors\"):\n",
    "            img_batch = torch.cat(imgs[i:i+batch_size], dim=0).to(device)  # Move batch to the device (GPU/CPU)\n",
    "            with torch.no_grad():\n",
    "                model = model.to(device)  # Ensure the model is on the correct device (GPU/CPU)\n",
    "                embedding_vectors = model(img_batch).to(device).numpy()  # Move the output back to the CPU\n",
    "                vectors.extend([embedding.flatten() for embedding in embedding_vectors])\n",
    "\n",
    "            torch.cuda.empty_cache()  # Clear memory after each batch\n",
    "\n",
    "    if vectors:\n",
    "        save_np = np.array(vectors)\n",
    "        np.save('save_np', save_np)\n",
    "        return save_np\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4575781e-af33-40d4-99d9-4fb12274b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_transform_image(artwork_path, transform, device):\n",
    "#     if not os.path.exists(artwork_path):\n",
    "#         # 이미지가 없으면 빈 벡터를 반환\n",
    "#         return np.zeros((512,))\n",
    "#     img = Image.open(artwork_path).convert('RGB')\n",
    "#     img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "#     return img_tensor\n",
    "\n",
    "# def compute_gallery_vector_batch(artworks, model, device, num_workers=4, batch_size=8):\n",
    "#     vectors = []\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ])\n",
    "\n",
    "#     imgs = []\n",
    "#     with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "#         img_futures = [executor.submit(load_and_transform_image, os.path.join(artwork), transform, device) for artwork in artworks]\n",
    "        \n",
    "#         for future in tqdm(img_futures, desc=\"Loading and transforming images\"):\n",
    "#             img_tensor = future.result()\n",
    "#             if isinstance(img_tensor, torch.Tensor):\n",
    "#                 imgs.append(img_tensor)\n",
    "\n",
    "#     if len(imgs) > 0:\n",
    "#         for i in range(0, len(imgs), batch_size):\n",
    "#             img_batch = torch.cat(imgs[i:i+batch_size], dim=0)\n",
    "#             with torch.no_grad():\n",
    "#                 with autocast():  # Mixed Precision 적용\n",
    "#                     embedding_vectors = model(img_batch).to(device).cpu().numpy()\n",
    "#                     vectors.extend([embedding.flatten() for embedding in embedding_vectors])\n",
    "#             torch.cuda.empty_cache()  # 메모리 비우기\n",
    "\n",
    "#     if vectors:\n",
    "#         save_np = np.array(vectors)\n",
    "#         np.save('save_np', save_np)\n",
    "#         return save_np\n",
    "#     else:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5cdb04a-bfec-4fb6-98e5-57e91362bbaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j-j11d106/.local/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = load_model(r'best_model(EfficientNetV2).pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "534e1dc5-ab34-4bbf-bbf7-ca96f9d4c610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading and transforming images: 100%|██████████| 80042/80042 [15:27<00:00, 86.26it/s] \n",
      "Loading Transfer vectors:   0%|          | 0/19837 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m user_gallery_vector \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gallery_vector_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m, in \u001b[0;36mcompute_gallery_vector_batch\u001b[0;34m(artworks, model, device, num_workers, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     29\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Ensure the model is on the correct device (GPU/CPU)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     embedding_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Move the output back to the CPU\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     vectors\u001b[38;5;241m.\u001b[39mextend([embedding\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m embedding_vectors])\n\u001b[1;32m     33\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()  \u001b[38;5;66;03m# Clear memory after each batch\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "user_gallery_vector = compute_gallery_vector_batch(list(data['filename']), model, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72117f53-59d5-4f25-81e6-ebbd3221057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_load = np.load('save_np.npy')\n",
    "np_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b498b-0a29-4369-88a4-b9df5b22f511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
